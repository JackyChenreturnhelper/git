# Latency 延遲 VsThroughput 吞吐量 

它們是衡量軟件系統的最常見的兩個指標。

延遲一般包括單向延遲（One-way Latency）和往返延遲（Round Trip Latency），實際測量時一般取往返延遲。它的單位一般是ms、s、min、h等。

而吞吐量一般指相當一段時間內測量出來的系統單位時間處理的任務數或事務數（TPS）。注意“相當一段時間”，不是幾秒，而可能是十幾分鐘、半個小時、一天、幾週甚至幾月。它的單位一般是TPS、每單位時間寫入磁盤的字節數等。
思考一個問題：

```低延遲一定意味著高吞吐量嗎？```

假如有一個網站系統，客戶端每次請求網站服務端，網絡傳輸時間（包括往返）為200ms，服務端處理請求為10ms。那麼如果是同步請求，則延遲為210ms。此時如果提高網絡傳輸速度，比如提高到100ms，那麼延遲為110ms。這種情況減少延遲似乎確實可以一定程度提高吞吐量，原因主要在於：系統性能瓶頸不在於服務端處理速度，而在於網絡傳輸速度。

    繼續假設將同步請求改為異步請求，那麼現在延遲為100ms，延遲降低了，但吞吐量保持不變。所以這是一個反例。


除了上面這個反例外，還有一個更生動的反例：

飛機 船送貨：從台灣到美國，飛機行駛20小時（包括往返）可以載貨100t，而火車行駛100小时（包括往返）可以載貨10000t

 顯然飛機載貨的延遲明顯低於船，但如果測試運10000t貨，則船運或的吞吐量遠高於飛機：

船運煤的吞吐量為100t/小時
飛機運煤的吞吐量為5t/小時
    我們可以將上面的運煤場景類比軟件系統，船、飛機運貨可以比作Web服務器處理請求，比如Apache和Nginx。在並發請求數不高時，比如10000（我假設的）以下時，也許Apache的吞吐量可能優於Nginx，但在大於10000時Apache的吞吐量就開始急劇下降，而Nginx的吞吐量相對之前比較穩定。所以比較Web服務器的吞吐量時，必須觀察在並發請求數逐漸遞增情況下它們各自的表現。

根據延遲和吞吐量我們還可以計算並發度（Concurrency），公式如下：

並發度　= 吞吐量 * 延遲


比如一個任務的處理花費1ms，吞吐量為1000tps，那麼並發度就等於1/1000*1000=1，可以得出任務處理線程模型是單線程模型。

## 吞吐量和延遲

### 下面的比喻是關於吞吐量（throughput）和延遲（latency）的。如果你要搞網絡性能優化，這兩個概念是你必須要知道的，它們看似簡單實則不是。我相信包括我在內的很多人都曾經認為大的吞吐量就意味著低延遲，高延遲就意味著吞吐量變小。下面的比喻可以解釋這種觀點根本不對。該比喻來自這裡，我來做個大體意譯（非逐字翻譯）。

### 我們可以把網絡發送數據包比喻成去街邊的ATM 取錢。每一個人從開始使用ATM 到取錢結束整個過程都需要一分鐘，所以這裡的延遲是60秒，那吞吐量呢？當然是1/60 人/秒。現在銀行升級了他們的ATM 機操作系統，每個人只要30秒就可以完成取款了！延遲是30秒，吞吐量是1/30 人/秒。很好理解，可是前面的問題依然存在對不對？別慌，看下面。

### 因為這附近來取錢的人比較多，現在銀行決定在這裡增加一台ATM 機，一共有兩台ATM 機了。現在，一分鐘可以讓4個人完成取錢了，雖然你去排隊取錢時在ATM 機前還是要用30 秒！也就是說，延遲沒有變，但吞吐量增大了！可見，吞吐量可以不用通過減小延遲來提高。

### 好了，現在銀行為了改進服務又做出了一個新的決定：每個來取錢的客戶在取完錢之後必須在旁邊填寫一個調查問卷，用時也是30秒。那麼，現在你去取錢的話從開始使用ATM 到完成調查問卷離開的時間又是60 秒了！換句話說，延遲是60秒。而吞吐量根本沒變！一分鐘之內還是可以進來4個人！可見，延遲增加了，而吞吐量沒有變。

### 從這個比喻中我們可以看出，延遲測量的是每個客戶（每個應用程序）感受到的時間長短，而吞吐量測量的是整個銀行（整個操作系統）的處理效率，是兩個完全不同的概念。用作者的原話說是：

```In short, the throughput is a function of how many stages are in parallel while latency is a function of how many are in series when there are multiple stages in the processing. The stage with the lowest throughput determines the overall throughput.```

### 正如銀行為了讓客戶滿意不光要提高自身的辦事效率外，還要盡量縮短客戶在銀行辦事所花的時間一樣，操作系統不光要盡量讓網絡吞吐量大，而且還要讓每個應用程序發送數據的延遲盡量小。這是兩個不同的目標。

# 降低latency

## 1 封包壓縮 (Gzip)

## 2 CDN

###  壓力測試

很多Web開發的朋友也經常討論Web應用如何有效的進行壓力測試，目前也沒有萬能的方法。可以使用的工具有loadrunner,或者Erlang語言開發的tsung等，很多公司也有自己的內部工具。HTTP/Memcache/MySQL等協議壓力測試其實相對簡單，通常用自己腳本或者高級語言開發的工具比起通用工具來說效果會更佳。

### profiling

對接口進行Profiling是發現瓶頸最直觀的方法，Google據說就有很完善的內部profiler工具(當然Google內部什麼工具都有)。我們討論了目前不同開發人員使用的profiling方法的優缺點。

1 直接使用專業工具，比如JProfiler, 還有Java自帶的JVisualVM等。

2 AOP(Aspect-oriented programming)的方式，優點是對程序沒有污染，在外部配置需要profiling的方法。

![apo](/images/AOP1.jpg)

3 工具類的方法，需要在service方法前後加入小量關鍵點，可以運行時動態打開或關閉profiler。比如通過給進程發signals的方法(見Signals and Java )動態讓程序輸出當前運行情況，起到了能夠動態profiling服務器但在正常情況下又不影響服務器性能的作用。

從討論情況來看大部分開發人員還是傾向於方法3，我們也希望團隊能逐步建立類似Google內部profiler之類自己的工具。

# 提高 Throughput 

一．系統吞度量要素：

一個系統的吞度量（承壓能力）與request對CPU的消耗、外部接口、IO等等緊密關聯。單個reqeust 對CPU消耗越高，外部系統接口、IO影響速度越慢，系統吞吐能力越低，反之越高。

系統吞吐量幾個重要參數：QPS（TPS）、並發數、響應時間
```
QPS（TPS）：每秒鐘request/事務數量

並發數： 系統同時處理的request/事務數

響應時間： 一般取平均響應時間
```

要提高一個服務的QPS，我們需要實現兩個指標：
降低這個鏈路的executeTime，以及提高cpu利用率。
降低鏈路的executeTime自然而然，我們把一個鏈路的同步調用，分成好幾段，原來需要經過三步才能拿到的返回結果，我們只需要經過一步就能拿到結果，那麼自然我們的性能就會提高很多。
至於提高cpu利用率。說到底其實就是一點，不要在計算的時候，老進行什麼IO、網絡讀寫，要計算就好好的計算，專人負責專事。

常見手段
1.非同步
即非阻塞，化繁為簡，拿到你需要處理的資源後儘快回复。適用於事務處理場景，且無需對上游返回數據場景。fature callback這種模式。
放行適當的流量，處理不了的請求直接返回錯誤或者其他提示。和水壩道理很類似

2 多台伺服器來運行服務

3.batch 批量處理
批量查詢、批量commit，基本上操作慢速設備或者不能並行化的對像或者資源時，使用batch 永遠是最好的手段。

4.設計
使用cache、靜態化等手段，其核心思想在於提前將結果準備好，實現的難點數據的更新。

請求盡量越前結束，越好，這樣壓力就不要穿透到後面的系統上，可以在各個層上加上緩存

1 Singleton

被實例化後就不會消失，程式運行期間只會有一個實例。

2 Scoped

每個 Request 都重新 new 一個新的實例，同一個 Request 不管經過多少個 Pipeline 都是用同一個實例。上例所使用的就是 Scoped。

3 Transient

每次注入時，都重新 new 一個新的實例。


優化數據庫，建立索引

5.Message queue 

訊息佇列移除元件之間的相依性，大幅地簡化去耦應用程式的編碼。軟體元件不會因為通訊程式碼而變得笨重，反而可以設計為執行獨立的商業功能，把複雜的事情丟給Message queue去做。



```
最後，要優化的地方還有很多，上面只是列舉常見一些要注意的地方，優化的指導原則就是

增加並發數和減少平均響應時間
```
